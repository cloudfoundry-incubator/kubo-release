#!/bin/bash -ex

NAME="${0##*/}"

export PATH=/var/vcap/packages/kubernetes/bin/:/var/vcap/packages/docker/sbin/:/var/vcap/packages/socat/bin/:$PATH

RUN_DIR=/var/vcap/sys/run/kubernetes
PIDFILE=$RUN_DIR/kubelet.pid
LOG_DIR=/var/vcap/sys/log/kubelet
DATA_DIR=/var/vcap/store/kubernetes
DOCKER_SOCKET=unix:///var/vcap/sys/run/docker/docker.sock

if [ -e /var/vcap/jobs/cloud-provider/bin/cloud-provider_utils ]; then
  . /var/vcap/jobs/cloud-provider/bin/cloud-provider_utils
  set_cloud_provider
  cloud_config="/var/vcap/jobs/cloud-provider/config/cloud-provider.ini"
else
  cloud_config=""
  cloud_provider=""
fi

<%
  labels = ["spec.ip=#{spec.ip}"]
  labels += p("labels").map {|k,v| "#{k}=#{v}"}
  labels = labels.join(',')
%>

# shellcheck disable=SC1091
. /var/vcap/packages/pid_utils/pid_utils.sh

setup_directories() {
  mkdir -p "$RUN_DIR" "$LOG_DIR" "$DATA_DIR"
  chown -R vcap:vcap "$RUN_DIR" "$LOG_DIR" "$DATA_DIR"
}

send_process_stdout_to_logfile() {
  exec 1>> "$LOG_DIR/$NAME.stdout.log"
}

send_process_stderr_to_logfile() {
  exec 2>> "$LOG_DIR/$NAME.stderr.log"
}

<% if_p('http_proxy') do |http_proxy| %>
export http_proxy=<%= http_proxy %>
<% end %>
<% if_p('https_proxy') do |https_proxy| %>
export https_proxy=<%= https_proxy %>
<% end %>
<% if_p('no_proxy') do |no_proxy| %>
export no_proxy=<%= no_proxy %>
# also use NO_PROXY since this fix is not in k8s we use yet: https://github.com/kubernetes/kubernetes/commit/92350f336b7aec303ba502c785d13f0cab1ad7f8#diff-1cb1608abe317a2c9acd24f738bc4837
export NO_PROXY=<%= no_proxy %>
<% end %>

start_kubelet() {

  ln -s -f /var/vcap/jobs/kubelet/packages/cni/bin/nsenter /usr/bin/nsenter

  if [[ "gce" == "$cloud_provider" ]]; then
    # The hostname needs to be set to the instance name so the gce cloud provider can manage the instance
    hostname_override=$(curl http://metadata.google.internal/computeMetadata/v1/instance/name -H "Metadata-Flavor: Google")
  else
    hostname_override=<%= spec.ip %>
  fi

  zone_label=""

  if [[ "vsphere" == "$cloud_provider"  ]]; then
    zone_label=",failure-domain.beta.kubernetes.io/zone=<%= spec.az %>"
  fi

  kubelet \
    --allow-privileged=<%= p('allow_privileged') %> \
    --cadvisor-port=0 \
    --cloud-config=${cloud_config} \
    --cloud-provider=${cloud_provider} \
    --cluster-dns=10.100.200.10 \
    --cluster-domain=cluster.local \
    --cni-bin-dir=/var/vcap/jobs/kubelet/packages/cni/bin \
    --container-runtime=docker \
    --docker-endpoint="${DOCKER_SOCKET}" \
    --docker="${DOCKER_SOCKET}" \
    --fail-swap-on=false \
    --hostname-override=${hostname_override} \
    --keep-terminated-pod-volumes=false \
    --kubeconfig=/var/vcap/jobs/kubelet/config/kubeconfig \
    --network-plugin=cni \
    --node-labels=<%= labels %>"$zone_label" \
    --read-only-port 0 \
    --serialize-image-pulls=false \
    --tls-cert-file=/var/vcap/jobs/kubelet/config/kubelet.pem \
    --tls-private-key-file=/var/vcap/jobs/kubelet/config/kubelet-key.pem \
    --v=<%=p('logging-level') %> \
    <% if_p('kube-reserved') do |kube_reserved| %>--kube-reserved="<%= kube_reserved %>"<% end %> \
    <% if_p('system-reserved') do |system_reserved| %>--system-reserved="<%= system_reserved %>"<% end %> \
    <% if_p('eviction-hard') do |eviction_hard| %>--eviction-hard="<%= eviction_hard %>"<% end %> \
  1>> $LOG_DIR/kubelet.stdout.log \
  2>> $LOG_DIR/kubelet.stderr.log
}

stop_kubelet() {
  kill_and_wait "$PIDFILE"
}

pid() {
  head -1 "$PIDFILE"
}

stop_associated_logging_processes() {
  # shellcheck disable=SC2046
  pkill -g $(get_group_pid)
}

get_group_pid() {
  ps -ho pgrp "$(pid)"
}


case $1 in

  start)

    setup_directories
    send_process_stdout_to_logfile
    send_process_stderr_to_logfile

    pid_guard "$PIDFILE" "Kubelet"

    echo $$ > $PIDFILE
    start_kubelet
    ;;

  stop)
    stop_associated_logging_processes
    stop_kubelet
    ;;

  *)
    echo "Usage: $0 {start|stop}"
    ;;

esac
